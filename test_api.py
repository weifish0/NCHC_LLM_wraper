#!/usr/bin/env python3
"""
NCHC LLM Wrapper API æ¸¬è©¦è…³æœ¬
"""

import requests
import json
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# API åŸºç¤ URL
BASE_URL = "http://localhost:8000"

def test_health_check():
    """æ¸¬è©¦å¥åº·æª¢æŸ¥ç«¯é»"""
    print("ğŸ” æ¸¬è©¦å¥åº·æª¢æŸ¥...")
    try:
        response = requests.get(f"{BASE_URL}/health")
        print(f"ç‹€æ…‹ç¢¼: {response.status_code}")
        print(f"å›æ‡‰: {response.json()}")
        return response.status_code == 200
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        return False

def test_root():
    """æ¸¬è©¦æ ¹è·¯å¾‘"""
    print("\nğŸ  æ¸¬è©¦æ ¹è·¯å¾‘...")
    try:
        response = requests.get(f"{BASE_URL}/")
        print(f"ç‹€æ…‹ç¢¼: {response.status_code}")
        print(f"å›æ‡‰: {response.json()}")
        return response.status_code == 200
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        return False

def test_models():
    """æ¸¬è©¦æ¨¡å‹åˆ—è¡¨ç«¯é»"""
    print("\nğŸ¤– æ¸¬è©¦æ¨¡å‹åˆ—è¡¨...")
    try:
        response = requests.get(f"{BASE_URL}/models")
        print(f"ç‹€æ…‹ç¢¼: {response.status_code}")
        print(f"å›æ‡‰: {response.json()}")
        return response.status_code == 200
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        return False

def test_simple_chat():
    """æ¸¬è©¦ç°¡åŒ–èŠå¤©ç«¯é»"""
    print("\nğŸ’¬ æ¸¬è©¦ç°¡åŒ–èŠå¤©...")
    
    # æª¢æŸ¥ API Key æ˜¯å¦è¨­å®š
    api_key = os.getenv("NCHC_API_KEY")
    if not api_key or api_key == "your_api_key_here":
        print("âš ï¸  API Key æœªè¨­å®šï¼Œè·³éèŠå¤©æ¸¬è©¦")
        return True
    
    try:
        payload = {
            "message": "ä½ å¥½ï¼Œè«‹ç°¡å–®ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±",
            "system_prompt": "ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ã€‚",
            "model": "Llama3-TAIDE-LX-8B-Chat-Alpha1"
        }
        
        response = requests.post(
            f"{BASE_URL}/chat/simple",
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=30
        )
        
        print(f"ç‹€æ…‹ç¢¼: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"å›æ‡‰: {result['response'][:100]}...")
            print(f"æ¨¡å‹: {result['model']}")
            print(f"ä½¿ç”¨é‡: {result['usage']}")
        else:
            print(f"éŒ¯èª¤å›æ‡‰: {response.text}")
        
        return response.status_code == 200
        
    except requests.exceptions.Timeout:
        print("â° è«‹æ±‚è¶…æ™‚")
        return False
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        return False

def test_full_chat_completions():
    """æ¸¬è©¦å®Œæ•´èŠå¤©å®Œæˆç«¯é»"""
    print("\nğŸ”„ æ¸¬è©¦å®Œæ•´èŠå¤©å®Œæˆ...")
    
    # æª¢æŸ¥ API Key æ˜¯å¦è¨­å®š
    api_key = os.getenv("NCHC_API_KEY")
    if not api_key or api_key == "your_api_key_here":
        print("âš ï¸  API Key æœªè¨­å®šï¼Œè·³éèŠå¤©æ¸¬è©¦")
        return True
    
    try:
        payload = {
            "model": "Llama3-TAIDE-LX-8B-Chat-Alpha1",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ã€‚"
                },
                {
                    "role": "user",
                    "content": "è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹å°ç£"
                }
            ],
            "max_tokens": 100,
            "temperature": 0.7
        }
        
        response = requests.post(
            f"{BASE_URL}/chat/completions",
            headers={"Content-Type": "application/json"},
            json=payload,
            timeout=30
        )
        
        print(f"ç‹€æ…‹ç¢¼: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"å›æ‡‰ ID: {result.get('id', 'N/A')}")
            print(f"æ¨¡å‹: {result.get('model', 'N/A')}")
            if result.get('choices'):
                content = result['choices'][0]['message']['content']
                print(f"å…§å®¹: {content}")
            print(f"ä½¿ç”¨é‡: {result.get('usage', {})}")
        else:
            print(f"éŒ¯èª¤å›æ‡‰: {response.text}")
        
        return response.status_code == 200
        
    except requests.exceptions.Timeout:
        print("â° è«‹æ±‚è¶…æ™‚")
        return False
    except Exception as e:
        print(f"éŒ¯èª¤: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ NCHC LLM Wrapper API")
    print("=" * 50)
    
    tests = [
        ("å¥åº·æª¢æŸ¥", test_health_check),
        ("æ ¹è·¯å¾‘", test_root),
        ("æ¨¡å‹åˆ—è¡¨", test_models),
        ("ç°¡åŒ–èŠå¤©", test_simple_chat),
        ("å®Œæ•´èŠå¤©å®Œæˆ", test_full_chat_completions)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ åŸ·è¡Œæ¸¬è©¦: {test_name}")
        print("-" * 30)
        
        if test_func():
            print(f"âœ… {test_name} æ¸¬è©¦é€šé")
            passed += 1
        else:
            print(f"âŒ {test_name} æ¸¬è©¦å¤±æ•—")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {passed}/{total} é€šé")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®šå’Œé€£ç·š")

if __name__ == "__main__":
    main() 